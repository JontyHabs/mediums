{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8c8ab5d",
   "metadata": {},
   "source": [
    "# Generating Petitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2b0e75",
   "metadata": {},
   "source": [
    "### Training our own Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f5dadb",
   "metadata": {},
   "source": [
    "For our first attempt, we'll train our own petition generator using a sample of ~15k UK Government petitions. We aim to create a model which generates text for a petition based on a short title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c481d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from typing import Iterable, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6c16b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "bucket = \"petition-data\"  # replace with your bucket name\n",
    "key = \"petitions.json\"    # replace with your filename\n",
    "\n",
    "s3client = boto3.client('s3')\n",
    "response = s3client.get_object(Bucket=bucket, Key= key)\n",
    "body = response['Body']\n",
    "\n",
    "data = json.loads(body.read())\n",
    "petitions = json.loads(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "021aeb84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': 'Many children and are choosing to live a life surrounded by the streets, drugs, and murders. This stems from poor education, the education system in the UK is outdated and boring. There needs to be a change to help children, give them confidence and knowledge of work skills.',\n",
       " 'title': 'Introduce work-based subjects and practical skills into the schools in the UK.',\n",
       " 'num_signatures': 127}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The petition data consists of a description, a title, and number of signatures\n",
    "display(petitions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4267adf6",
   "metadata": {},
   "source": [
    "PyTorch uses Datasets and DataLoaders. Datasets are classes which allow individual samples to be retrieved, DataLoaders take care of batches. Models take DataLoaders as input.\n",
    "\n",
    "A custom Dataset class must implement __init__ and __len__. A standard \"map style\" dataset must implement __getitem__, whilst an IterableDataset must implement __iter__.\n",
    "\n",
    "https://medium.com/speechmatics/how-to-build-a-streaming-dataloader-with-pytorch-a66dd891d9dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d27b790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom Dataset object\n",
    "class IterablePetitionsDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, data):\n",
    "        super(IterablePetitionsDataset).__init__()\n",
    "        self.data = data\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.data)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "# Organise our data to be a list of samples, each containing the petition title\n",
    "# on the left (input) and description on the right (output)\n",
    "data = [(p['title'], p['description']) for p in petitions]\n",
    "\n",
    "# Split the data into train and validation (N.B the order of the data is already random)\n",
    "data_valid = data[:2500]\n",
    "data_train = data[2500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7088162",
   "metadata": {},
   "source": [
    "Next, we need to create our vocabulary from our training data. We'll also include special symbols in our vocab, which are expected by PyTorch later:\n",
    "\n",
    "unk - 'unknown' - used in inference when an input word isn't in our vocabulary\n",
    "\n",
    "pad - 'padding' - used to pad our word sequences to ensure consistent length\n",
    "\n",
    "bos - 'beginning of string' - used to denote start of a sequence\n",
    "\n",
    "eos - 'end of string' - used to denote end of a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba42a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "token_transform = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "# Function to yield a list of words from an iterable of text samples\n",
    "def yield_tokens(data_iter: Iterable) -> List[str]:\n",
    "    for data_sample in data_iter:\n",
    "        # Use both the input and output texts (title and description)\n",
    "        # to generate our vocab\n",
    "        text = \" \".join([data_sample[0], data_sample[1]])\n",
    "        yield token_transform(text)\n",
    "\n",
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "# Create vocab from training data\n",
    "train_iter = IterablePetitionsDataset(data_train)\n",
    "vocab_transform = build_vocab_from_iterator(yield_tokens(train_iter),\n",
    "                                                    min_freq=1,\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True)\n",
    "\n",
    "\n",
    "# Set UNK_IDX as the default index. This index is returned when the token is not found.\n",
    "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary.\n",
    "vocab_transform.set_default_index(UNK_IDX)\n",
    "\n",
    "# We now have a vocab - this is just a lookup which translates words into integers and vice versa\n",
    "display(vocab_transform.lookup_indices([\"Translate\", \"tokens\", \"to\", \"numbers\"]))\n",
    "display(vocab_transform.lookup_indices([0, 29713, 6, 1037]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b592eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c290739",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "# Seq2Seq Network\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = Transformer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a179af49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d85947c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "VOCAB_SIZE = len(vocab_transform)\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 128 #64\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, VOCAB_SIZE, VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f5a606",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                      torch.tensor(token_ids),\n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and tgt language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "text_transform = sequential_transforms(token_transform, #Tokenization\n",
    "                                               vocab_transform, #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tesors\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(text_transform(src_sample.rstrip(\"\\n\")))\n",
    "        tgt_batch.append(text_transform(tgt_sample.rstrip(\"\\n\")))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a9c367",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = IterablePetitionsDataset(data_train)\n",
    "# train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f88c723",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf7b2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "#     train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    train_iter = IterablePetitionsDataset(data_train)\n",
    "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in train_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(train_dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "#     val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    val_iter = IterablePetitionsDataset(data_valid)\n",
    "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in val_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e1d7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 30 #10 #18 #5\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "\n",
    "\n",
    "# function to generate output sequence using greedy algorithm\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "\n",
    "# actual function to translate input sentence into target language\n",
    "def translate(model: torch.nn.Module, src_sentence: str):\n",
    "    model.eval()\n",
    "    src = text_transform(src_sentence).view(-1, 1)\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(\n",
    "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "    return \" \".join(vocab_transform.lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c494f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model: torch.nn.Module, src_sentence: str):\n",
    "    model.eval()\n",
    "    src = text_transform(src_sentence).view(-1, 1)\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(\n",
    "        model,  src, src_mask, max_len=num_tokens + 100, start_symbol=BOS_IDX).flatten()\n",
    "    return \" \".join(vocab_transform.lookup_tokens(list(tgt_tokens.cpu().numpy())))#.replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n",
    "\n",
    "print(translate(transformer, \"Data should be taught in schools.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92029a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(translate(transformer, \"Ban plastic bags.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd563d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(translate(transformer, \"Introduce work-based subjects and practical skills into the schools in the UK.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599cdafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(translate(transformer, \"Unicorns are dangerous and must be stopped.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321f5fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model: torch.nn.Module, src_sentence: str):\n",
    "    model.eval()\n",
    "    src = text_transform(src_sentence).view(-1, 1)\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(\n",
    "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "    return \" \".join(vocab_transform.lookup_tokens(list(tgt_tokens.cpu().numpy())))#.replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n",
    "\n",
    "print(translate(transformer, \"Make data science mandatory\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb656532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out model\n",
    "import io\n",
    "buffer = io.BytesIO()\n",
    "torch.save(transformer.state_dict(), buffer)\n",
    "conn.put_object(Bucket=\"torch-models\", Key=\"petition_001.json\", Body=buffer.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eacbce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out model\n",
    "torch.save(transformer.state_dict(), 'petiton_transformer_001')\n",
    "response = s3client.upload_file('petiton_transformer_001', 'torch-models', 'petiton_transformer_001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c521fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out model\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "args, _ = parser.parse_known_args()\n",
    "    \n",
    "with open(os.path.join(args.model_dir, 'model.pth'), 'wb') as f:\n",
    "    torch.save(model.state_dict(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0870d611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload model\n",
    "\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "import json\n",
    "role = get_execution_role()\n",
    "\n",
    "conn = boto3.client('s3')\n",
    "conn.download_file('torch-models', 'petiton_transformer_001', 'temp.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dda0fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, VOCAB_SIZE, VOCAB_SIZE, FFN_HID_DIM)\n",
    "loaded_model.load_state_dict(torch.load('temp.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3869b0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate output sequence using greedy algorithm\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "def translate(model: torch.nn.Module, src_sentence: str):\n",
    "    model.eval()\n",
    "    src = text_transform(src_sentence).view(-1, 1)\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(\n",
    "        model,  src, src_mask, max_len=num_tokens + 100, start_symbol=BOS_IDX).flatten()\n",
    "    return \" \".join(vocab_transform.lookup_tokens(list(tgt_tokens.cpu().numpy())))#.replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n",
    "\n",
    "DEVICE = torch.device('cpu')\n",
    "print(translate(loaded_model, \"Rejoin the EU.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10af2914",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(translate(loaded_model, \"Give Yorkshire independence.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a467a576",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f6cb3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e66d2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e02292d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = s3client.get_object(Bucket='torch-models', Key=\"petiton_transformer_001\")\n",
    "body = response['Body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf547732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9434a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "body.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2793f56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test reloading\n",
    "loaded_model = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, VOCAB_SIZE, VOCAB_SIZE, FFN_HID_DIM)\n",
    "loaded_model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee6af94",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dumps(transformer.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f52ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert your existing model to JSON\n",
    "saved_model = model.to_json()\n",
    "\n",
    "# Write JSON object to S3 as \"model.json\"\n",
    "client = boto3.client('s3')\n",
    "client.put_object(Body=saved_model,\n",
    "                  Bucket='BUCKET_NAME',\n",
    "                  Key='model.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1ce616",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_valid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1174af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(translate(transformer, \"Introduce work-based subjects and practical skills into the schools in the UK.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3986a135",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(translate(transformer, \"We should teach data science in school\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58c07bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4c9024",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(translate(transformer, \"Drei Kartoffeln sind rot\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e237b18c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bd3525",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "a = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "# for src, tgt in a:\n",
    "#     print(src)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4f90b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for thing in a:\n",
    "    src, tgt = thing\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227d9ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "src.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2896e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "a = list(pd.DataFrame(tgt)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b26f683",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_transform[TGT_LANGUAGE].lookup_tokens(list(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf01bdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = list(pd.DataFrame(src)[0])\n",
    "vocab_transform[SRC_LANGUAGE].lookup_tokens(list(b))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
