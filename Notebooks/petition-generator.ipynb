{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0976b40c",
   "metadata": {},
   "source": [
    "# Generating Petitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de66c35a",
   "metadata": {},
   "source": [
    "### Training our own Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab8112d",
   "metadata": {},
   "source": [
    "For our first attempt, we'll train our own petition generator using a sample of ~15k UK Government petitions. We aim to create a model which generates text for a petition based on a short title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b9737ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import math\n",
    "from timeit import default_timer as timer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Iterable, List\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch import Tensor\n",
    "from torch.nn import Transformer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1927733d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': 'Many children and are choosing to live a life surrounded by the streets, drugs, and murders. This stems from poor education, the education system in the UK is outdated and boring. There needs to be a change to help children, give them confidence and knowledge of work skills.',\n",
       " 'title': 'Introduce work-based subjects and practical skills into the schools in the UK.',\n",
       " 'num_signatures': 127}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the data\n",
    "bucket = \"petition-data\"  # replace with your bucket name\n",
    "key = \"petitions.json\"    # replace with your filename\n",
    "\n",
    "s3client = boto3.client('s3')\n",
    "response = s3client.get_object(Bucket=bucket, Key= key)\n",
    "body = response['Body']\n",
    "\n",
    "data = json.loads(body.read())\n",
    "petitions = json.loads(data)\n",
    "\n",
    "# The petition data consists of a description, a title, and number of signatures\n",
    "display(petitions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db1f5f8",
   "metadata": {},
   "source": [
    "PyTorch uses Datasets and DataLoaders. Datasets are classes which allow individual samples to be retrieved, DataLoaders take care of batches. Models take DataLoaders as input.\n",
    "\n",
    "A custom Dataset class must implement __init__ and __len__. A standard \"map style\" dataset must implement __getitem__, whilst an IterableDataset must implement __iter__.\n",
    "\n",
    "https://medium.com/speechmatics/how-to-build-a-streaming-dataloader-with-pytorch-a66dd891d9dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82003f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom Dataset object\n",
    "class IterablePetitionsDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, data):\n",
    "        super(IterablePetitionsDataset).__init__()\n",
    "        self.data = data\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.data)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "# Organise our data to be a list of samples, each containing the petition title\n",
    "# on the left (input) and description on the right (output)\n",
    "data = [(p['title'], p['description']) for p in petitions]\n",
    "\n",
    "# Split the data into train and validation (N.B the order of the data is already random)\n",
    "data_valid = data[:2500]\n",
    "data_train = data[2500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af728499",
   "metadata": {},
   "source": [
    "Next, we need to create our vocabulary from our training data. We'll also include special symbols in our vocab, which are expected by PyTorch later:\n",
    "\n",
    "unk - 'unknown' - used in inference when an input word isn't in our vocabulary\n",
    "\n",
    "pad - 'padding' - used to pad our word sequences to ensure consistent length\n",
    "\n",
    "bos - 'beginning of string' - used to denote start of a sequence\n",
    "\n",
    "eos - 'end of string' - used to denote end of a sequence\n",
    "\n",
    "We won't do any case normalisation, stemming, stop-word removal, etc. These steps are useful in some language tasks, but when to generate realistic text we need to be able to generate full words, stop words, and upper case when appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c615854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 29713, 6, 1037]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['<unk>', 'tokens', 'to', 'numbers']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create tokenizer\n",
    "token_transform = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "# Function to yield a list of words from an iterable of text samples\n",
    "def yield_tokens(data_iter: Iterable) -> List[str]:\n",
    "    for data_sample in data_iter:\n",
    "        # Use both the input and output texts (title and description)\n",
    "        # to generate our vocab\n",
    "        text = \" \".join([data_sample[0], data_sample[1]])\n",
    "        yield token_transform(text)\n",
    "\n",
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "# Create vocab from training data\n",
    "train_iter = IterablePetitionsDataset(data_train)\n",
    "vocab_transform = build_vocab_from_iterator(yield_tokens(train_iter),\n",
    "                                                    min_freq=1,\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True)\n",
    "\n",
    "\n",
    "# Set UNK_IDX as the default index. This index is returned when the token is not found.\n",
    "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary.\n",
    "vocab_transform.set_default_index(UNK_IDX)\n",
    "\n",
    "# We now have a vocab - this is just a lookup which translates words into integers and vice versa\n",
    "display(vocab_transform.lookup_indices([\"Translate\", \"tokens\", \"to\", \"numbers\"]))\n",
    "display(vocab_transform.lookup_tokens([0, 29713, 6, 1037]))"
   ]
  },
  {
   "attachments": {
    "The-transformer-model-from-Attention-is-all-you-need-Viswani-et-al_Q320.jpg": {
     "image/jpeg": "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"
    }
   },
   "cell_type": "markdown",
   "id": "a240e8b1",
   "metadata": {},
   "source": [
    "![The-transformer-model-from-Attention-is-all-you-need-Viswani-et-al_Q320.jpg](attachment:The-transformer-model-from-Attention-is-all-you-need-Viswani-et-al_Q320.jpg)\n",
    "\n",
    "Now it's time to code the main elements of the Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5973cd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure you run on a GPU if possible\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Input/Output Embedding #\n",
    "# A module to embed a sequence - i.e. transform each word in the sequence into a vector\n",
    "# This will give us a tensor representing the vector for each word in the sequence\n",
    "# We'll use this module on each side of the Transformer (pink boxes above)\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "# Positional Encoding #\n",
    "# A Module to add positional encoding to the token embedding in order to codify word order\n",
    "# This is just calculating the sinusoids and adding the resulting tensor to the embedding tensor.\n",
    "# There is also a standard dropout layer, and a line to save the positional embeddings\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "\n",
    "\n",
    "# The full Transformer!\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = Transformer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceded1b1",
   "metadata": {},
   "source": [
    "That Seq2SeqTransformer looks complicated, but the main bit is in the forward method. We apply token (word) embedding to the source and target sequences, followed by positional encoding. We then feed them both into the inbuilt Transformer module, along with some masks (which we'll get to next).\n",
    "\n",
    "The encode and decode methods allow us to apply just the encoder or decoder parts of the architecture - these will be used after training to actually use the model. As per the architecure diagram, there is also a Linear layer topping off the Transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635e3481",
   "metadata": {},
   "source": [
    "Next we need to make two kinds of mask. One mask hides any padding tokens - we don't want to predict padding! The other mask hides the tokens in the target sequence which are right of the token we are predicting. i.e. - we don't want our model peeking at the answers while it's training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0dd00ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee83c9ea",
   "metadata": {},
   "source": [
    "Next we define some parameters, such as the size of our token embeddings, the size of our batches, the number of encoder and decoder blocks we want to stack up, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2711228",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "VOCAB_SIZE = len(vocab_transform)\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, VOCAB_SIZE, VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b60e668",
   "metadata": {},
   "source": [
    "Finally, we need some helper functions which we'll call when transforming a Dataset instance into a DataLoader instance. These will transform the raw sentences in our training data into tensors. The sentence will be tokenized (split into words), numericalized (each word is turned into a number), and then made into a tensor (during which we'll also add **bos** and **eos** tokens). We'll also pad out our sequences to ensure all source sequences in a given batch are equal length, and the same for target seuqnces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2661c526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                      torch.tensor(token_ids),\n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and tgt language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "text_transform = sequential_transforms(token_transform, #Tokenization\n",
    "                                               vocab_transform, #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tesors\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(text_transform(src_sample.rstrip(\"\\n\")))\n",
    "        tgt_batch.append(text_transform(tgt_sample.rstrip(\"\\n\")))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a8a22d",
   "metadata": {},
   "source": [
    "Almost there! We need a function to train each epoch - this just turns out training data into batches of tensors (using the functions above), and then puts each batch through the model to update the model weights. A very similar function puts our validation through the model to calculate loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74be710e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    train_iter = IterablePetitionsDataset(data_train)\n",
    "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in train_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(train_dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    val_iter = IterablePetitionsDataset(data_valid)\n",
    "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in val_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f65f76",
   "metadata": {},
   "source": [
    "And now we train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccb9e0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 8.074, Val loss: 6.982, Epoch time = 50.101s\n",
      "Epoch: 2, Train loss: 6.777, Val loss: 6.500, Epoch time = 52.665s\n",
      "Epoch: 3, Train loss: 6.284, Val loss: 6.153, Epoch time = 54.492s\n",
      "Epoch: 4, Train loss: 5.979, Val loss: 5.984, Epoch time = 54.301s\n",
      "Epoch: 5, Train loss: 5.781, Val loss: 5.880, Epoch time = 54.385s\n",
      "Epoch: 6, Train loss: 5.630, Val loss: 5.819, Epoch time = 54.436s\n",
      "Epoch: 7, Train loss: 5.501, Val loss: 5.776, Epoch time = 54.405s\n",
      "Epoch: 8, Train loss: 5.386, Val loss: 5.740, Epoch time = 54.226s\n",
      "Epoch: 9, Train loss: 5.279, Val loss: 5.715, Epoch time = 54.433s\n",
      "Epoch: 10, Train loss: 5.181, Val loss: 5.692, Epoch time = 54.364s\n",
      "Epoch: 11, Train loss: 5.085, Val loss: 5.682, Epoch time = 54.485s\n",
      "Epoch: 12, Train loss: 4.988, Val loss: 5.670, Epoch time = 54.406s\n",
      "Epoch: 13, Train loss: 4.897, Val loss: 5.660, Epoch time = 54.510s\n",
      "Epoch: 14, Train loss: 4.814, Val loss: 5.671, Epoch time = 54.519s\n",
      "Epoch: 15, Train loss: 4.732, Val loss: 5.661, Epoch time = 54.502s\n",
      "Epoch: 16, Train loss: 4.649, Val loss: 5.645, Epoch time = 54.473s\n",
      "Epoch: 17, Train loss: 4.563, Val loss: 5.639, Epoch time = 54.683s\n",
      "Epoch: 18, Train loss: 4.482, Val loss: 5.651, Epoch time = 54.518s\n",
      "Epoch: 19, Train loss: 4.407, Val loss: 5.655, Epoch time = 54.658s\n",
      "Epoch: 20, Train loss: 4.324, Val loss: 5.659, Epoch time = 54.422s\n",
      "Epoch: 21, Train loss: 4.242, Val loss: 5.657, Epoch time = 54.434s\n",
      "Epoch: 22, Train loss: 4.170, Val loss: 5.702, Epoch time = 54.618s\n",
      "Epoch: 23, Train loss: 4.099, Val loss: 5.749, Epoch time = 54.620s\n",
      "Epoch: 24, Train loss: 4.027, Val loss: 5.742, Epoch time = 54.534s\n",
      "Epoch: 25, Train loss: 3.953, Val loss: 5.724, Epoch time = 54.547s\n",
      "Epoch: 26, Train loss: 3.879, Val loss: 5.750, Epoch time = 54.534s\n",
      "Epoch: 27, Train loss: 3.802, Val loss: 5.756, Epoch time = 54.481s\n",
      "Epoch: 28, Train loss: 3.729, Val loss: 5.795, Epoch time = 54.604s\n",
      "Epoch: 29, Train loss: 3.657, Val loss: 5.808, Epoch time = 54.592s\n",
      "Epoch: 30, Train loss: 3.587, Val loss: 5.847, Epoch time = 54.643s\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 30\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "03d1529e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out model\n",
    "torch.save(transformer.state_dict(), 'petiton_transformer_002')\n",
    "response = s3client.upload_file('petiton_transformer_002', 'torch-models', 'petiton_transformer_002')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7a4bb8",
   "metadata": {},
   "source": [
    "### Stepping through the model - validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4252cf",
   "metadata": {},
   "source": [
    "The training and validation can be quite mystifying - so let's take a single example through the model, end-to-end, to understand what's happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "8e2346b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Introduce work-based subjects and practical skills into the schools in the UK.',\n",
       " 'Many children and are choosing to live a life surrounded by the streets, drugs, and murders. This stems from poor education, the education system in the UK is outdated and boring. There needs to be a change to help children, give them confidence and knowledge of work skills.')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's take an example from the validation data:\n",
    "datapoint = data_valid[0]\n",
    "\n",
    "display(datapoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "0869196f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([   2,  238,  107,   26,  357, 2047,    7, 3239,  923,   96,    4,  125,\n",
       "          11,    4,   19,    5,    3])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['<bos>',\n",
       " 'Introduce',\n",
       " 'work',\n",
       " '-',\n",
       " 'based',\n",
       " 'subjects',\n",
       " 'and',\n",
       " 'practical',\n",
       " 'skills',\n",
       " 'into',\n",
       " 'the',\n",
       " 'schools',\n",
       " 'in',\n",
       " 'the',\n",
       " 'UK',\n",
       " '.',\n",
       " '<eos>']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 1: Translate a sentence into scalars\n",
    "\n",
    "src_sentence = datapoint[0]\n",
    "tgt_sentence = datapoint[1]\n",
    "\n",
    "# Use our tokenizer and lookup from the start to change a sentence into words, and\n",
    "# those words into integers\n",
    "src_vec = text_transform(src_sentence)\n",
    "tgt_vec = text_transform(tgt_sentence)\n",
    "\n",
    "# Each input word is now a single number, and we also have spots for the full stop,\n",
    "# <bos> and <eos>\n",
    "display(src_vec.shape)\n",
    "display(src_vec)\n",
    "display(vocab_transform.lookup_tokens(list(src_vec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "e1e8fbd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([56, 56])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf]], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[  2., 201., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[  2., 201.,  58., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 2: Prepare our inputs and outputs, and create masks\n",
    "\n",
    "# Convert our 1D vector to a 2D tensor; in a real run, this is unneccessary\n",
    "# as the DataLoader returns the right dimensions\n",
    "src = src_vec.unsqueeze(1)\n",
    "tgt = tgt_vec.unsqueeze(1)\n",
    "\n",
    "# Place onto the same device as the model\n",
    "src = src.to(DEVICE)\n",
    "tgt = tgt.to(DEVICE)\n",
    "\n",
    "# We have to cut off the end token when inputting the target sequence, as there\n",
    "# is no token after the final token for us to predict.\n",
    "tgt_input = tgt[:-1, :]\n",
    "\n",
    "# Create our masks\n",
    "src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "# As we have a batch size of 1, both padding masks are redundant (we have no\n",
    "# padding). The target mask is the important one - it is essentially a series of mask vectors.\n",
    "# The first vector hides everything except the first token, so that we can predict the \n",
    "# second word. The second vector reveals the first two words, etc.\n",
    "display(tgt_mask.shape)\n",
    "\n",
    "for i in range(0,3):\n",
    "    masked_target = tgt_input.reshape(1, tgt_mask.shape[0]) + tgt_mask[i]\n",
    "    display(masked_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "fe59f1b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([56, 1, 30315])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "30315"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 3: Get our logits\n",
    "\n",
    "# Before running model, set some hooks\n",
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "transformer.src_tok_emb.register_forward_hook(get_activation('src_tok_emb'))\n",
    "transformer.tgt_tok_emb.register_forward_hook(get_activation('tgt_tok_emb'))\n",
    "\n",
    "# Get logits\n",
    "logits = transformer(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "# The resulting tensor gives us, for every masked variant of the input, a score for every possible\n",
    "# next token. See that the tensor is size 56 (for the 56 target tokens, and therefore 56 masked variants), \n",
    "# by 1 (for a batch size of 1), by 30315 (the size of our vocab, to allow a probability per possible token)\n",
    "display(logits.shape)\n",
    "display(len(vocab_transform))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c6abf8",
   "metadata": {},
   "source": [
    "What happened int hat last step? Our model took both our source sequence and target sequence, and outputted a set of un-normalised probabilities (logits). To do this, the model applied token and positional embedding to both source and target, and took both sequences through the various Linear layers and Attention modules.\n",
    "\n",
    "Let's dig into these logits a bit more. I can pick the indices with the 5 greatest scores to determine the 3 most likely words to come after *bos*, for my given input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "fcea28ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'As', 'I', 'We', 'Many']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# All pretty standard words - all capitalised as you'd expect for a starting word\n",
    "display(vocab_transform.lookup_tokens(list(torch.topk(logits[0], 5).indices.tolist()[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed251b6",
   "metadata": {},
   "source": [
    "I can look at a longer part of the target sequence to see what the next word might be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "69d03b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Masked target: ['<bos>']\n",
      "Most propable next word: ['The', 'As', 'I', 'We', 'Many']\n",
      "\n",
      "Masked target: ['<bos>', 'Many']\n",
      "Most propable next word: ['schools', 'students', 'parents', 'young', 'children']\n",
      "\n",
      "Masked target: ['<bos>', 'Many', 'children']\n",
      "Most propable next word: ['are', 'in', 'do', 'have', 'across']\n",
      "\n",
      "Masked target: ['<bos>', 'Many', 'children', 'and']\n",
      "Most propable next word: ['school', 'young', 'their', 'parents', 'children']\n",
      "\n",
      "Masked target: ['<bos>', 'Many', 'children', 'and', 'are']\n",
      "Most propable next word: ['being', 'forced', 'not', 'expected', 'in']\n"
     ]
    }
   ],
   "source": [
    "def display_masked_target_and_predictions(i):\n",
    "    masked_target = tgt_input.reshape(1, tgt_mask.shape[0]) + tgt_mask[i]\n",
    "    unmasked_target_sequence = masked_target[0].tolist()[:i+1]\n",
    "    unmasked_target_sequence = [int(x) for x in unmasked_target_sequence]\n",
    "\n",
    "    print(\"\\nMasked target: {}\"\n",
    "          .format(vocab_transform.lookup_tokens(unmasked_target_sequence)))\n",
    "\n",
    "    print(\"Most propable next word: {}\"\n",
    "          .format(vocab_transform.lookup_tokens(list(torch.topk(logits[i], 5).indices.tolist()[0]))))\n",
    "    \n",
    "display_masked_target_and_predictions(0)\n",
    "display_masked_target_and_predictions(1)\n",
    "display_masked_target_and_predictions(2)\n",
    "display_masked_target_and_predictions(3)\n",
    "display_masked_target_and_predictions(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4020a2b",
   "metadata": {},
   "source": [
    "We can see that the first prediction is not bad - the right answer is 5th most likely out of over 30k words. Same again for the next word. The predictins lean towards themes of school and children even before any such terms are revealed in the target sequence, so this is picked up from the source sequence, and the top predictions at each step make grammatical sense. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "72f20019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.8950, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Calculate loss\n",
    "\n",
    "# Finally, we calculate the cross entropy loss. The logits are raw values, not propbabilities,\n",
    "# but the inbuilt cross entropy loss function from PyTorch applies a Softmax behind the scenes.\n",
    "\n",
    "loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e465d74",
   "metadata": {},
   "source": [
    "And just out of interest, you can even get the embeddings of the source and target sequences (see that for each input word, we have a 512-length vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "e11778f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 1, 512])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([56, 1, 512])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(activation['src_tok_emb'].shape)\n",
    "display(activation['tgt_tok_emb'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50d4b17",
   "metadata": {},
   "source": [
    "### Writing petitions\n",
    "\n",
    "It's finally time to write some petitions!\n",
    "\n",
    "As we saw before, we can predict probabilities for a \"next\" word based on both the entire source sequence and the target sequence up to that point. For creating new text, we essentialy do the same thing. Starting with a target that is simply **bos**, we predict probabilities for all possible first words. We then select the most likely word, and repeat the process, this time with a target that includes that chosen first word. We continue doing this until we either hit a maximum sequence length, or the token **eos** is predicted.\n",
    "\n",
    "This approach is called greedy decoding. There are other options which make a tree of possible sequences and choose the most likely branch overall, but greedy is simplest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "c607e669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate output sequence using greedy algorithm\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "# Actual function to translate input sentence into target language\n",
    "def translate(model: torch.nn.Module, src_sentence: str):\n",
    "    model.eval()\n",
    "    src = text_transform(src_sentence).view(-1, 1)\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(\n",
    "        model,  src, src_mask, max_len=num_tokens + 100, start_symbol=BOS_IDX).flatten()\n",
    "    return \" \".join(vocab_transform.lookup_tokens(list(tgt_tokens.cpu().numpy())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e6c0f2",
   "metadata": {},
   "source": [
    "First, let's try our previous example from the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "159b29d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Actual title: Introduce work-based subjects and practical skills into the schools in the UK.\n",
      "\n",
      "Actual description: Many children and are choosing to live a life surrounded by the streets, drugs, and murders. This stems from poor education, the education system in the UK is outdated and boring. There needs to be a change to help children, give them confidence and knowledge of work skills.\n",
      "\n",
      "Predicted description: <bos> The current curriculum for students , and school are not allowed to teach their children about their school , however , we are not taught about how to teach our children 's education . This is a problem that we do n't know what is a GCSE and we do n't know how to teach our children ? <eos>\n"
     ]
    }
   ],
   "source": [
    "datapoint = data_valid[0]\n",
    "\n",
    "print(\"\\nActual title: \" + datapoint[0])\n",
    "print(\"\\nActual description: \" + datapoint[1])\n",
    "print(\"\\nPredicted description: \" + translate(transformer, datapoint[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466f5ec5",
   "metadata": {},
   "source": [
    "Not bad! The prediction sticks to talking about education and mostly makes grammatical sense. It doesn't make a whole lot of **actual** sense, but that's what happens with a relatively small model and pretty small training data. Let's try another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "1f62037f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Actual title: Give EU citizens working in the NHS free indefinite leave to remain in the UK.\n",
      "\n",
      "Actual description: The possibility of a \"no-deal\" Brexit has been described as uncomfortably high. \n",
      "There are a significant number of EU citizens working in the NHS.  Any disruption in their working rights may have catastrophic effects. The government should give them indefinite leave to remain for free.\n",
      "\n",
      "\n",
      "Predicted description: <bos> The UK should not be able to leave the EU after Brexit , as they are entitled to a referendum . \n",
      " If we leave the EU , we can not afford to pay for free prescriptions , we should not be able to pay to pay for free movement of people who are entitled to pay . <eos>\n"
     ]
    }
   ],
   "source": [
    "datapoint = data_valid[16]\n",
    "\n",
    "print(\"\\nActual title: \" + datapoint[0])\n",
    "print(\"\\nActual description: \" + datapoint[1])\n",
    "print(\"\\nPredicted description: \" + translate(transformer, datapoint[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fcf237",
   "metadata": {},
   "source": [
    "Again, not too bad. We can definitely see some ropey ones though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "359afc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Actual title: To save the 16 remaining hospital beds at Okehampton Hospital\n",
      "\n",
      "Actual description: The Clinical Commission Group are proposing to remove the 16 remaining hospital beds at Okehampton Hospital. Residents of Okehampton and the surrounding area, urge the retention of our local hospital beds. Okehampton is the hub of a huge rural area with a population of 20,000 people. .\n",
      "\n",
      "Predicted description: <bos> The government has announced the government to cut the funding of the town of the town and the town of the town . \n",
      " The government has to cut the funding of the town and the closure of the town of the town has to cut the town of the town . <eos>\n"
     ]
    }
   ],
   "source": [
    "datapoint = data_valid[12]\n",
    "\n",
    "print(\"\\nActual title: \" + datapoint[0])\n",
    "print(\"\\nActual description: \" + datapoint[1])\n",
    "print(\"\\nPredicted description: \" + translate(transformer, datapoint[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3607508",
   "metadata": {},
   "source": [
    "This kind of repetition is a pretty classic issue in generating long text. Finally, let's try to make up some new titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "7eb673ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos> The government should provide a better education for students to be able to provide their children for their children . <eos>'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'<bos> I believe that the current law should be a legal requirement to protect children from the country and learn about how to learn how much of the country is a good form of abuse . <eos>'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'<bos> The UK government must provide the use of human rights to protect animals from entering the UK to provide access to the use of development to use the use of development which allows the use of development to use their products to use by banning their use . <eos>'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(translate(transformer, \"All children should learn computer science.\"))\n",
    "display(translate(transformer, \"Artificial intelligence should rule the country.\"))\n",
    "display(translate(transformer, \"The country should be ruled by AI.\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
